#!/usr/bin/env python
# coding=utf-8

"""
@author: zgw
@date: 2025/5/15 17:03
@source from: 
"""
from tqdm import tqdm
import json
from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, Dataset
import torch
from torch import nn

# è·¯å¾„æ›¿æ¢æˆä½ è‡ªå·±çš„
train_path = '/Volumes/PSSD/NetThink/pythonProject/7-19-Project/TCM-Syndrome-and-Disease-Differentiation/data/train.json'
test_path = '/Volumes/PSSD/NetThink/pythonProject/7-19-Project/TCM-Syndrome-and-Disease-Differentiation/data/test.json'
val_path = '/Volumes/PSSD/NetThink/pythonProject/7-19-Project/TCM-Syndrome-and-Disease-Differentiation/data/test.json'
model_path = '/Volumes/mac_win/models/tiansz/bert-base-chinese'

class MyDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_length=512):
        self.encodings = tokenizer(texts, truncation=True, padding='max_length',#token_type_idsä»£è¡¨çš„æ˜¯
                                   max_length=max_length, return_tensors="pt")#åªè¦æ˜¯ç»è¿‡bertçš„tokenizeréƒ½ä¼šå˜æˆä¸‰ä¸ªå‘é‡ã€input_idsã€token_type_idsã€attention_mask
        self.labels = torch.tensor(labels)

    def __getitem__(self, idx):
        # for key,val in self.encodings.items():
        #     print(key)
        #     print(val[idx])
        #     print('debug')
        item = {key: val[idx] for key, val in self.encodings.items()}#å®ƒæ˜¯éå†ä¸Šè¿°å­—å…¸çš„ä¸€ç§æ–¹å¼ã€‚å°†å…¶å˜æˆå­—å…¸ã€‚
        item["labels"] = self.labels[idx]
        '''
        æœ€ç»ˆitemå˜æˆä¸ºï¼š
        item = {
            "input_ids":[2,512],
            "token_type_ids":[2,512],
            "attention_mask":[2,512],
            "labels":[2,4]
        }
        '''
        return item

    def __len__(self):
        return len(self.labels)


def TCM_SD_Data_Loader(tokenizer, batch_size=2):
    syndromes = ['èƒ¸ç—¹å¿ƒç—›ç—…', 'å¿ƒè¡°ç—…', 'çœ©æ™•ç—…', 'å¿ƒæ‚¸ç—…']
    syndrome2id_dict = {v: i for i, v in enumerate(syndromes)}
    id2syndrome_dict = {i: v for i, v in enumerate(syndromes)}

    def load_data(path):
        with open(path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        texts, labels = [], []
        for content in tqdm(data, desc=f'Loading {path}'):
            text = content['ç—‡çŠ¶'] + content['ä¸­åŒ»æœ›é—»åˆ‡è¯Š']
            label = syndrome2id_dict[content['ç–¾ç—…']]  # ğŸŸ¢ changed to integer
            texts.append(text)
            labels.append(label)
        return texts, labels

    train_texts, train_labels = load_data(train_path)
    test_texts, test_labels = load_data(test_path)
    val_texts, val_labels = load_data(val_path)

    train_dataset = MyDataset(train_texts, train_labels, tokenizer)
    test_dataset = MyDataset(test_texts, test_labels, tokenizer)
    val_dataset = MyDataset(val_texts, val_labels, tokenizer)


    return train_dataset, test_dataset, val_dataset, id2syndrome_dict

import numpy as np
from sklearn.metrics import accuracy_score, f1_score
from transformers import EvalPrediction

def compute_metrics(eval_pred: EvalPrediction):
    logits, labels = eval_pred.predictions, eval_pred.label_ids
    # å°† one-hot æ ‡ç­¾è½¬ä¸ºæ•´æ•°ç±»åˆ«ï¼Œå¦‚ [0, 1, 0, 0] -> 1
    #labels = np.argmax(labels, axis=1)
    preds = np.argmax(logits, axis=1)

    return {
        "accuracy": accuracy_score(labels, preds),
        "f1_macro": f1_score(labels, preds, average="macro"),
        "f1_weighted": f1_score(labels, preds, average="weighted")
    }
from transformers import AutoModelForSequenceClassification,AutoTokenizer
from transformers import Trainer, TrainingArguments
from transformers import BertTokenizer, BertForSequenceClassification

tokenizer = BertTokenizer.from_pretrained(model_path)
model = BertForSequenceClassification.from_pretrained(model_path, num_labels=4)
train_dataset, test_dataset, val_dataset, id2syndrome_dict = TCM_SD_Data_Loader(tokenizer)
print('debug')
# éå† dataset ä¸­æ¯ä¸€æ¡æ•°æ®
# for idx in range(len(train_dataset)):
#         sample = train_dataset[idx]
#         print(f"Sample {idx}:")
#         print("  input_ids:", sample["input_ids"])
#         print("  token_type_ids:", sample["token_type_ids"])
#         print("  attention_mask:", sample["attention_mask"])
#         print("  labels:", sample["labels"])
#         print("-" * 30)
training_args = TrainingArguments(
    output_dir="./checkpoints",
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    num_train_epochs=5,
    logging_dir='./logs',
    load_best_model_at_end=True,
    metric_for_best_model='accuracy',
    no_cuda=True,  # âœ… å¼ºåˆ¶åªä½¿ç”¨ CPU
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics  # è¿”å› accuracy ç­‰æŒ‡æ ‡
)
trainer.train()



'''
output_dir="./checkpoints"
æ¨¡å‹ä¿å­˜çš„è¾“å‡ºè·¯å¾„ï¼Œä¼šä¿å­˜æ¯ä¸ª epoch ç”Ÿæˆçš„æƒé‡ï¼ˆåŒ…æ‹¬æœ€ä¼˜æ¨¡å‹ï¼‰ã€‚


per_device_train_batch_size=8
æ¯ä¸ªè®¾å¤‡ï¼ˆGPU/CPUï¼‰ä¸Šçš„è®­ç»ƒ batch sizeã€‚Mac æ²¡æœ‰ GPUï¼Œè¿™é‡Œå°±æ˜¯æ¯æ¬¡è®­ç»ƒç”¨ 8 ä¸ªæ ·æœ¬ã€‚


per_device_eval_batch_size=8
æ¯ä¸ªè®¾å¤‡ä¸ŠéªŒè¯æ—¶çš„ batch sizeï¼ŒåŒæ ·æ˜¯ 8 ä¸ªæ ·æœ¬ã€‚


evaluation_strategy="epoch"
è¯„ä¼°ç­–ç•¥ï¼Œæ¯ä¸ª epoch ç»“æŸåè‡ªåŠ¨åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°ä¸€æ¬¡ã€‚


save_strategy="epoch"
æ¨¡å‹ä¿å­˜ç­–ç•¥ï¼Œæ¯ä¸ª epoch ä¿å­˜ä¸€æ¬¡ checkpointï¼ˆåŒ…å« model/optimizer/scheduler çŠ¶æ€ï¼‰ã€‚


num_train_epochs=5
æ¨¡å‹è®­ç»ƒçš„æ€»è½®æ•°ï¼ˆepochï¼‰ã€‚


logging_dir='./logs'
è®­ç»ƒè¿‡ç¨‹çš„æ—¥å¿—ç›®å½•ï¼ˆæ¯”å¦‚ TensorBoard å¯è§†åŒ–æ—¥å¿—ï¼‰ã€‚


load_best_model_at_end=True
æ˜¯å¦åœ¨è®­ç»ƒç»“æŸååŠ è½½éªŒè¯é›†ä¸Šè¡¨ç°æœ€å¥½çš„æ¨¡å‹ã€‚éœ€è¦ metric_for_best_model é…åˆã€‚


metric_for_best_model='accuracy'
åˆ¤æ–­â€œæœ€ä¼˜æ¨¡å‹â€æ‰€ä½¿ç”¨çš„è¯„ä¼°æŒ‡æ ‡ï¼Œè¿™é‡Œæ˜¯ 'accuracy'ï¼ˆä½ éœ€è¦è‡ªå·±å®šä¹‰ compute_metrics è¿”å›è¿™ä¸ªå­—æ®µï¼‰ã€‚


no_cuda=True
âœ… ç¦ç”¨ CUDAï¼Œåªç”¨ CPU è®­ç»ƒï¼Œé€‚ç”¨äº Mac æˆ–æ—  GPU æƒ…å†µã€‚


learning_rate
float
å­¦ä¹ ç‡ï¼Œé»˜è®¤ 5e-5ï¼Œå¸¸ç”¨äºå¾®è°ƒã€‚


weight_decay
float
æƒé‡è¡°å‡ (L2æ­£åˆ™)ï¼Œé€šå¸¸è®¾ä¸º 0.01ã€‚


adam_beta1 / adam_beta2
float
Adam ä¼˜åŒ–å™¨çš„ Î² å‚æ•°ï¼Œé»˜è®¤ (0.9, 0.999)ã€‚


adam_epsilon
float
é˜²æ­¢é™¤é›¶çš„å°æ•°ï¼Œé»˜è®¤ 1e-8ã€‚


max_grad_norm
float
æ¢¯åº¦è£å‰ªé˜ˆå€¼ï¼Œé¿å…æ¢¯åº¦çˆ†ç‚¸ï¼Œé»˜è®¤ 1.0ã€‚


lr_scheduler_type
str
å­¦ä¹ ç‡è°ƒåº¦å™¨ç±»å‹ï¼šlinearã€cosineã€constant ç­‰ã€‚


warmup_steps
int
è®­ç»ƒå‰å¤šå°‘æ­¥è¿›è¡Œå­¦ä¹ ç‡ warmupï¼ˆçº¿æ€§å‡é«˜ï¼‰ï¼Œé€‚åˆå¤§æ¨¡å‹æˆ–ä½å­¦ä¹ ç‡æƒ…å†µã€‚


gradient_accumulation_steps
int
æ¢¯åº¦ç´¯è®¡æ­¥æ•°ï¼Œç”¨äº batch size å¤ªå°æ—¶â€œæ¨¡æ‹Ÿâ€æ›´å¤§çš„ batchã€‚




'''

